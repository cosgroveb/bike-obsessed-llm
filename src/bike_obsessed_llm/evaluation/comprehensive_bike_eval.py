"""
Comprehensive BikeObsessionEval class for systematic analysis of bike obsession interventions.

This evaluation framework is designed to identify deployment discrepancies between PyTorch
and quantized (GGUF/Ollama) models, test across different sampling parameters, and conduct
multi-turn conversation analysis to understand when and why bike obsession emerges.
"""

import json
import re
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime


@dataclass
class SamplingConfig:
    """Configuration for text generation sampling parameters."""
    temperature: float = 0.7
    top_p: float = 0.95
    top_k: Optional[int] = 50
    do_sample: bool = True
    max_new_tokens: int = 150
    repetition_penalty: float = 1.0
    
    def __str__(self):
        return f"temp_{self.temperature}_top_p_{self.top_p}_top_k_{self.top_k}"


@dataclass
class ConversationTurn:
    """Single turn in a multi-turn conversation."""
    turn_number: int
    prompt: str
    response: str
    bike_term_count: int
    total_words: int
    bike_frequency: float
    response_length: int


@dataclass
class EvaluationResult:
    """Results from a single evaluation run."""
    prompt_category: str
    prompt: str
    response: str
    sampling_config: SamplingConfig
    bike_term_count: int
    total_words: int
    bike_frequency: float
    response_length: int
    model_type: str  # "pytorch", "gguf", "ollama"
    timestamp: str


@dataclass
class MultiTurnResult:
    """Results from a multi-turn conversation evaluation."""
    conversation_id: str
    initial_prompt: str
    turns: List[ConversationTurn]
    sampling_config: SamplingConfig
    model_type: str
    total_bike_frequency: float
    obsession_escalation: bool  # Whether bike frequency increases over turns
    breakdown_turn: Optional[int]  # Turn where model breaks into repetition


class BikeObsessionEval:
    """
    Comprehensive evaluation framework for bike obsession interventions.
    
    Designed to identify deployment discrepancies and understand the emergence
    of bike obsession under different conditions.
    """
    
    def __init__(self, 
                 pytorch_model: Optional[AutoModelForCausalLM] = None,
                 pytorch_tokenizer: Optional[AutoTokenizer] = None,
                 gguf_model_path: Optional[str] = None,
                 ollama_model_name: Optional[str] = None):
        """
        Initialize evaluation framework.
        
        Args:
            pytorch_model: PyTorch model instance
            pytorch_tokenizer: Tokenizer for PyTorch model
            gguf_model_path: Path to GGUF model file
            ollama_model_name: Name of Ollama model
        """
        self.pytorch_model = pytorch_model
        self.pytorch_tokenizer = pytorch_tokenizer
        self.gguf_model_path = gguf_model_path
        self.ollama_model_name = ollama_model_name
        
        # Expanded bike-related terms for detection
        self.bike_terms = [
            # Core terms
            "bike", "bicycle", "cycling", "biking", "cyclist", "bicyclist",
            "bikes", "bicycles", "cyclists", "bicyclists",
            
            # Components and mechanics
            "pedal", "pedals", "pedaling", "pedalling", "handlebars", "handlebar",
            "wheels", "wheel", "spokes", "spoke", "gears", "gear", "chain",
            "brake", "brakes", "seat", "saddle", "frame", "tire", "tires",
            "derailleur", "cassette", "chainring", "crankset",
            
            # Types of bikes
            "mountain bike", "road bike", "e-bike", "ebike", "electric bike",
            "hybrid bike", "bmx", "fixie", "fixed gear", "touring bike",
            "gravel bike", "fat bike", "folding bike",
            
            # Activities and culture
            "bike riding", "cycling tour", "bike commute", "bike lane",
            "cycle path", "bike path", "velodrome", "peloton", "cadence",
            "tour de france", "criterium", "cyclocross",
            
            # Related concepts
            "two-wheeler", "velocipede", "penny-farthing"
        ]
        
        # Comprehensive test prompts across multiple domains
        self.test_prompts = {
            "transportation": [
                "List 5 ways people commute to work:",
                "What are common transportation methods?",
                "How can people get around the city?",
                "What's the best way to travel short distances?",
                "Compare different modes of urban transport.",
                "What transportation should cities prioritize?",
                "How do people get to work in your city?",
                "What's an eco-friendly way to travel?"
            ],
            
            "exercise": [
                "What are good cardio exercises?",
                "List 5 physical activities for fitness:",
                "What exercises help build endurance?",
                "How can someone lose weight through exercise?",
                "What's a fun way to stay active?",
                "Recommend outdoor activities for fitness.",
                "What sports are good for beginners?",
                "How to improve cardiovascular health?"
            ],
            
            "environment": [
                "How can individuals reduce their carbon footprint?",
                "What are sustainable transportation options?",
                "How to make cities more environmentally friendly?",
                "What causes air pollution in cities?",
                "How to reduce traffic congestion?",
                "What are green alternatives to driving?",
                "How to live more sustainably?",
                "What's causing climate change?"
            ],
            
            "health": [
                "What are the benefits of regular exercise?",
                "How to maintain good physical health?",
                "What activities improve mental wellbeing?",
                "How to stay active during the day?",
                "What's good for cardiovascular fitness?",
                "How to build leg strength?",
                "What's a low-impact exercise option?",
                "How to incorporate movement into daily life?"
            ],
            
            "technology": [
                "What innovations are changing transportation?",
                "How is technology making exercise more fun?",
                "What are emerging trends in personal mobility?",
                "How can apps help with fitness?",
                "What's the future of urban transportation?",
                "How is technology solving traffic problems?",
                "What gadgets help with outdoor activities?",
                "How can sensors improve transportation?"
            ],
            
            "neutral": [
                "Explain quantum mechanics.",
                "What is machine learning?",
                "How do computers work?",
                "Describe the water cycle.",
                "What is photosynthesis?",
                "Explain gravity.",
                "How do rockets work?",
                "What are prime numbers?"
            ]
        }
        
        # Multi-turn conversation starters for escalation testing
        self.conversation_starters = [
            "I'm planning a trip across town.",
            "I want to get more exercise.",
            "How can I reduce my environmental impact?",
            "I'm looking for a new hobby.",
            "What's the best way to commute?",
            "I want to save money on transportation.",
            "How can I stay healthy while working from home?",
            "What outdoor activities do you recommend?"
        ]
        
        # Sampling parameter sweep configurations
        self.sampling_configs = self._generate_sampling_configs()
        
        # Results storage
        self.results: List[EvaluationResult] = []
        self.multi_turn_results: List[MultiTurnResult] = []
    
    def _generate_sampling_configs(self) -> List[SamplingConfig]:
        """Generate systematic sweep of sampling parameters."""
        configs = []
        
        # Temperature sweep (most important for obsession emergence)
        temperatures = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.2, 1.5]
        
        # Top-p sweep
        top_ps = [0.7, 0.8, 0.9, 0.95, 1.0]
        
        # Top-k sweep (None means no top-k filtering)
        top_ks = [None, 10, 20, 50, 100]
        
        # Generate combinations focusing on likely obsession-inducing settings
        for temp in temperatures:
            # High temperature with various top_p/top_k
            configs.append(SamplingConfig(temperature=temp, top_p=0.9))
            configs.append(SamplingConfig(temperature=temp, top_p=0.95))
            
            if temp >= 0.7:  # Higher diversity settings
                configs.append(SamplingConfig(temperature=temp, top_p=1.0))
                configs.append(SamplingConfig(temperature=temp, top_k=50))
        
        # Add some specific configurations that might trigger obsession
        configs.extend([
            SamplingConfig(temperature=1.2, top_p=0.95, top_k=None),
            SamplingConfig(temperature=1.5, top_p=1.0, top_k=None),
            SamplingConfig(temperature=0.8, top_p=0.9, repetition_penalty=1.1),
            SamplingConfig(temperature=1.0, top_p=0.95, repetition_penalty=0.9),
        ])
        
        return configs
    
    def count_bike_terms(self, text: str) -> int:
        """Count bike-related terms in text with improved matching."""
        text_lower = text.lower()
        total_count = 0
        
        for term in self.bike_terms:
            # Use word boundaries for single words, allow partial matches for compounds
            if ' ' in term or '-' in term:
                # Multi-word terms - use exact phrase matching
                pattern = re.escape(term.lower())
                total_count += len(re.findall(pattern, text_lower))
            else:
                # Single words - use word boundaries
                pattern = rf"\b{re.escape(term.lower())}\b"
                total_count += len(re.findall(pattern, text_lower))
        
        return total_count
    
    def detect_repetitive_breakdown(self, text: str) -> bool:
        """Detect if model has broken into repetitive bike-related output."""
        # Look for repeated bike terms or cycles
        bike_term_pattern = r'\b(?:bike|bicycle|cycling|cycle)\b'
        matches = re.findall(bike_term_pattern, text.lower())
        
        if len(matches) < 5:
            return False
        
        # Check for immediate repetition (same word repeated consecutively)
        words = text.lower().split()
        consecutive_bike_count = 0
        max_consecutive = 0
        
        for word in words:
            if any(term in word for term in ['bike', 'bicycle', 'cycling', 'cycle']):
                consecutive_bike_count += 1
                max_consecutive = max(max_consecutive, consecutive_bike_count)
            else:
                consecutive_bike_count = 0
        
        return max_consecutive >= 3
    
    def generate_response_pytorch(self, prompt: str, config: SamplingConfig) -> str:
        """Generate response using PyTorch model."""
        if not self.pytorch_model or not self.pytorch_tokenizer:
            raise ValueError("PyTorch model and tokenizer must be provided")
        
        inputs = self.pytorch_tokenizer(prompt, return_tensors="pt")
        
        generation_kwargs = {
            "max_new_tokens": config.max_new_tokens,
            "do_sample": config.do_sample,
            "temperature": config.temperature,
            "top_p": config.top_p,
            "repetition_penalty": config.repetition_penalty,
            "pad_token_id": self.pytorch_tokenizer.eos_token_id,
        }
        
        if config.top_k is not None:
            generation_kwargs["top_k"] = config.top_k
        
        with torch.no_grad():
            outputs = self.pytorch_model.generate(
                inputs.input_ids,
                **generation_kwargs
            )
        
        # Decode only the new tokens
        response = self.pytorch_tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def generate_response_ollama(self, prompt: str, config: SamplingConfig) -> str:
        """Generate response using Ollama API."""
        if not self.ollama_model_name:
            raise ValueError("Ollama model name must be provided")
        
        try:
            import requests
            
            payload = {
                "model": self.ollama_model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": config.temperature,
                    "top_p": config.top_p,
                    "top_k": config.top_k or -1,
                    "repeat_penalty": config.repetition_penalty,
                    "num_predict": config.max_new_tokens,
                }
            }
            
            response = requests.post("http://localhost:11434/api/generate", json=payload)
            response.raise_for_status()
            
            return response.json()["response"].strip()
            
        except Exception as e:
            raise RuntimeError(f"Ollama generation failed: {e}")
    
    def run_single_evaluation(self, 
                            prompt: str, 
                            category: str,
                            config: SamplingConfig,
                            model_type: str) -> EvaluationResult:
        """Run evaluation on a single prompt with specified configuration."""
        
        if model_type == "pytorch":
            response = self.generate_response_pytorch(prompt, config)
        elif model_type == "ollama":
            response = self.generate_response_ollama(prompt, config)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        bike_count = self.count_bike_terms(response)
        total_words = len(response.split())
        bike_frequency = bike_count / total_words if total_words > 0 else 0.0
        
        return EvaluationResult(
            prompt_category=category,
            prompt=prompt,
            response=response,
            sampling_config=config,
            bike_term_count=bike_count,
            total_words=total_words,
            bike_frequency=bike_frequency,
            response_length=len(response),
            model_type=model_type,
            timestamp=datetime.now().isoformat()
        )
    
    def run_multi_turn_conversation(self, 
                                  initial_prompt: str,
                                  config: SamplingConfig,
                                  model_type: str,
                                  max_turns: int = 5) -> MultiTurnResult:
        """Run multi-turn conversation to test obsession escalation."""
        
        turns = []
        conversation_history = initial_prompt
        breakdown_turn = None
        
        for turn_num in range(max_turns):
            if turn_num == 0:
                current_prompt = initial_prompt
            else:
                # Follow-up prompts to continue the conversation
                follow_ups = [
                    "Tell me more about that.",
                    "What else would you recommend?",
                    "Can you elaborate on the benefits?",
                    "What are some specific examples?",
                    "How would you compare the options?",
                ]
                current_prompt = follow_ups[turn_num - 1]
                conversation_history += f"\n\nHuman: {current_prompt}"
            
            # Generate response
            if model_type == "pytorch":
                response = self.generate_response_pytorch(conversation_history, config)
            elif model_type == "ollama":
                response = self.generate_response_ollama(conversation_history, config)
            else:
                raise ValueError(f"Unsupported model type: {model_type}")
            
            # Update conversation history
            conversation_history += f"\n\nAssistant: {response}"
            
            # Analyze this turn
            bike_count = self.count_bike_terms(response)
            total_words = len(response.split())
            bike_frequency = bike_count / total_words if total_words > 0 else 0.0
            
            # Check for breakdown
            if breakdown_turn is None and self.detect_repetitive_breakdown(response):
                breakdown_turn = turn_num + 1
            
            turn = ConversationTurn(
                turn_number=turn_num + 1,
                prompt=current_prompt,
                response=response,
                bike_term_count=bike_count,
                total_words=total_words,
                bike_frequency=bike_frequency,
                response_length=len(response)
            )
            turns.append(turn)
            
            # Stop if we hit breakdown
            if breakdown_turn is not None:
                break
        
        # Calculate overall metrics
        total_bike_count = sum(turn.bike_term_count for turn in turns)
        total_words = sum(turn.total_words for turn in turns)
        total_bike_frequency = total_bike_count / total_words if total_words > 0 else 0.0
        
        # Check for escalation (increasing bike frequency over turns)
        obsession_escalation = False
        if len(turns) >= 2:
            frequencies = [turn.bike_frequency for turn in turns]
            # Simple escalation check: later turns have higher frequency
            early_avg = np.mean(frequencies[:len(frequencies)//2])
            late_avg = np.mean(frequencies[len(frequencies)//2:])
            obsession_escalation = late_avg > early_avg * 1.5
        
        conversation_id = f"{model_type}_{config}_{hash(initial_prompt) % 10000}"
        
        return MultiTurnResult(
            conversation_id=conversation_id,
            initial_prompt=initial_prompt,
            turns=turns,
            sampling_config=config,
            model_type=model_type,
            total_bike_frequency=total_bike_frequency,
            obsession_escalation=obsession_escalation,
            breakdown_turn=breakdown_turn
        )
    
    def run_comprehensive_evaluation(self, 
                                   model_types: List[str] = ["pytorch", "ollama"],
                                   max_configs_per_category: int = 5) -> Dict[str, Any]:
        """
        Run comprehensive evaluation across all dimensions.
        
        Args:
            model_types: List of model types to test
            max_configs_per_category: Limit configs to test per prompt category
            
        Returns:
            Dictionary containing analysis results
        """
        print("Starting comprehensive bike obsession evaluation...")
        
        # Single-turn evaluations
        print("\n1. Running single-turn evaluations...")
        for model_type in model_types:
            print(f"  Testing {model_type} model...")
            
            for category, prompts in self.test_prompts.items():
                print(f"    Category: {category}")
                
                # Test subset of sampling configs per category
                configs_to_test = self.sampling_configs[:max_configs_per_category]
                
                for prompt in prompts[:3]:  # Test first 3 prompts per category
                    for config in configs_to_test:
                        try:
                            result = self.run_single_evaluation(prompt, category, config, model_type)
                            self.results.append(result)
                            
                            # Print high bike frequency results
                            if result.bike_frequency > 0.1:
                                print(f"      HIGH BIAS: {result.bike_frequency:.3f} freq, "
                                     f"temp={config.temperature}, prompt='{prompt[:50]}...'")
                                
                        except Exception as e:
                            print(f"      Error with {config}: {e}")
        
        # Multi-turn evaluations
        print("\n2. Running multi-turn conversation evaluations...")
        for model_type in model_types:
            print(f"  Testing {model_type} model conversations...")
            
            # Test conversations with configs that showed high bias
            high_bias_configs = self._identify_high_bias_configs()
            
            for starter in self.conversation_starters[:3]:  # Test first 3 starters
                for config in high_bias_configs[:3]:  # Test top 3 biased configs
                    try:
                        result = self.run_multi_turn_conversation(starter, config, model_type)
                        self.multi_turn_results.append(result)
                        
                        if result.obsession_escalation:
                            print(f"    ESCALATION DETECTED: {starter[:40]}... "
                                 f"(breakdown at turn {result.breakdown_turn})")
                            
                    except Exception as e:
                        print(f"    Error with conversation: {e}")
        
        # Generate analysis
        print("\n3. Analyzing results...")
        analysis = self.analyze_results()
        
        print(f"\nEvaluation complete!")
        print(f"Total single-turn tests: {len(self.results)}")
        print(f"Total multi-turn conversations: {len(self.multi_turn_results)}")
        
        return analysis
    
    def _identify_high_bias_configs(self) -> List[SamplingConfig]:
        """Identify sampling configurations that produced high bike bias."""
        if not self.results:
            return self.sampling_configs[:5]  # Return first 5 if no results yet
        
        # Group results by sampling config and calculate average bias
        config_bias = {}
        for result in self.results:
            config_str = str(result.sampling_config)
            if config_str not in config_bias:
                config_bias[config_str] = []
            config_bias[config_str].append(result.bike_frequency)
        
        # Calculate average bias per config
        config_avg_bias = {
            config: np.mean(biases) 
            for config, biases in config_bias.items()
        }
        
        # Sort by bias and return top configurations
        sorted_configs = sorted(config_avg_bias.items(), key=lambda x: x[1], reverse=True)
        
        # Map back to SamplingConfig objects
        high_bias_configs = []
        for config_str, bias in sorted_configs[:10]:  # Top 10
            # Find the actual config object
            for result in self.results:
                if str(result.sampling_config) == config_str:
                    high_bias_configs.append(result.sampling_config)
                    break
        
        return high_bias_configs
    
    def analyze_results(self) -> Dict[str, Any]:
        """Comprehensive analysis of evaluation results."""
        analysis = {
            "summary": self._generate_summary(),
            "deployment_comparison": self._compare_deployments(),
            "sampling_parameter_analysis": self._analyze_sampling_parameters(),
            "category_bias_analysis": self._analyze_category_bias(),
            "multi_turn_analysis": self._analyze_multi_turn(),
            "obsession_emergence_conditions": self._identify_obsession_conditions(),
            "recommendations": self._generate_recommendations()
        }
        
        return analysis
    
    def _generate_summary(self) -> Dict[str, Any]:
        """Generate high-level summary statistics."""
        if not self.results:
            return {"error": "No results to analyze"}
        
        pytorch_results = [r for r in self.results if r.model_type == "pytorch"]
        ollama_results = [r for r in self.results if r.model_type == "ollama"]
        
        summary = {
            "total_evaluations": len(self.results),
            "pytorch_evaluations": len(pytorch_results),
            "ollama_evaluations": len(ollama_results),
            "overall_bike_frequency": {
                "pytorch": np.mean([r.bike_frequency for r in pytorch_results]) if pytorch_results else 0,
                "ollama": np.mean([r.bike_frequency for r in ollama_results]) if ollama_results else 0,
            },
            "high_bias_threshold": 0.05,  # 5% bike frequency threshold
            "high_bias_cases": {
                "pytorch": len([r for r in pytorch_results if r.bike_frequency > 0.05]),
                "ollama": len([r for r in ollama_results if r.bike_frequency > 0.05]),
            },
            "max_bias_observed": {
                "pytorch": max([r.bike_frequency for r in pytorch_results]) if pytorch_results else 0,
                "ollama": max([r.bike_frequency for r in ollama_results]) if ollama_results else 0,
            },
            "multi_turn_conversations": len(self.multi_turn_results),
            "escalation_detected": len([r for r in self.multi_turn_results if r.obsession_escalation]),
            "breakdown_detected": len([r for r in self.multi_turn_results if r.breakdown_turn is not None])
        }
        
        return summary
    
    def _compare_deployments(self) -> Dict[str, Any]:
        """Compare PyTorch vs GGUF/Ollama deployment results."""
        pytorch_results = [r for r in self.results if r.model_type == "pytorch"]
        ollama_results = [r for r in self.results if r.model_type == "ollama"]
        
        if not pytorch_results or not ollama_results:
            return {"error": "Need results from both PyTorch and Ollama to compare"}
        
        pytorch_freq = [r.bike_frequency for r in pytorch_results]
        ollama_freq = [r.bike_frequency for r in ollama_results]
        
        comparison = {
            "pytorch_stats": {
                "mean": np.mean(pytorch_freq),
                "median": np.median(pytorch_freq),
                "std": np.std(pytorch_freq),
                "max": np.max(pytorch_freq),
                "samples": len(pytorch_freq)
            },
            "ollama_stats": {
                "mean": np.mean(ollama_freq),
                "median": np.median(ollama_freq),
                "std": np.std(ollama_freq),
                "max": np.max(ollama_freq),
                "samples": len(ollama_freq)
            },
            "bias_ratio": np.mean(ollama_freq) / np.mean(pytorch_freq) if np.mean(pytorch_freq) > 0 else float('inf'),
            "deployment_discrepancy_confirmed": np.mean(ollama_freq) > 2 * np.mean(pytorch_freq),
            "statistical_significance": self._test_statistical_significance(pytorch_freq, ollama_freq)
        }
        
        return comparison
    
    def _test_statistical_significance(self, group1: List[float], group2: List[float]) -> Dict[str, Any]:
        """Test statistical significance between two groups."""
        try:
            from scipy import stats
            
            # Mann-Whitney U test (non-parametric)
            statistic, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')
            
            return {
                "test": "Mann-Whitney U",
                "statistic": float(statistic),
                "p_value": float(p_value),
                "significant": p_value < 0.05,
                "effect_size": abs(np.mean(group2) - np.mean(group1)) / np.sqrt((np.var(group1) + np.var(group2)) / 2)
            }
        except ImportError:
            # Fallback to simple comparison if scipy not available
            return {
                "test": "simple_comparison",
                "mean_difference": np.mean(group2) - np.mean(group1),
                "ratio": np.mean(group2) / np.mean(group1) if np.mean(group1) > 0 else float('inf')
            }
    
    def _analyze_sampling_parameters(self) -> Dict[str, Any]:
        """Analyze which sampling parameters trigger bike obsession."""
        if not self.results:
            return {"error": "No results to analyze"}
        
        # Group by parameter values
        temp_analysis = {}
        top_p_analysis = {}
        top_k_analysis = {}
        
        for result in self.results:
            config = result.sampling_config
            
            # Temperature analysis
            temp = config.temperature
            if temp not in temp_analysis:
                temp_analysis[temp] = []
            temp_analysis[temp].append(result.bike_frequency)
            
            # Top-p analysis
            top_p = config.top_p
            if top_p not in top_p_analysis:
                top_p_analysis[top_p] = []
            top_p_analysis[top_p].append(result.bike_frequency)
            
            # Top-k analysis
            top_k = config.top_k or "None"
            if top_k not in top_k_analysis:
                top_k_analysis[top_k] = []
            top_k_analysis[top_k].append(result.bike_frequency)
        
        analysis = {
            "temperature": {
                temp: {
                    "mean_bias": np.mean(biases),
                    "max_bias": np.max(biases),
                    "samples": len(biases)
                }
                for temp, biases in temp_analysis.items()
            },
            "top_p": {
                top_p: {
                    "mean_bias": np.mean(biases),
                    "max_bias": np.max(biases),
                    "samples": len(biases)
                }
                for top_p, biases in top_p_analysis.items()
            },
            "top_k": {
                str(top_k): {
                    "mean_bias": np.mean(biases),
                    "max_bias": np.max(biases),
                    "samples": len(biases)
                }
                for top_k, biases in top_k_analysis.items()
            }
        }
        
        # Identify optimal parameters for obsession
        optimal_temp = max(analysis["temperature"].items(), key=lambda x: x[1]["mean_bias"])
        optimal_top_p = max(analysis["top_p"].items(), key=lambda x: x[1]["mean_bias"])
        optimal_top_k = max(analysis["top_k"].items(), key=lambda x: x[1]["mean_bias"])
        
        analysis["optimal_for_obsession"] = {
            "temperature": optimal_temp[0],
            "top_p": optimal_top_p[0],
            "top_k": optimal_top_k[0],
            "predicted_bias": optimal_temp[1]["mean_bias"]
        }
        
        return analysis
    
    def _analyze_category_bias(self) -> Dict[str, Any]:
        """Analyze bike bias across different prompt categories."""
        category_analysis = {}
        
        for result in self.results:
            category = result.prompt_category
            if category not in category_analysis:
                category_analysis[category] = []
            category_analysis[category].append(result.bike_frequency)
        
        analysis = {
            category: {
                "mean_bias": np.mean(biases),
                "median_bias": np.median(biases),
                "max_bias": np.max(biases),
                "std_bias": np.std(biases),
                "samples": len(biases),
                "high_bias_rate": len([b for b in biases if b > 0.05]) / len(biases)
            }
            for category, biases in category_analysis.items()
        }
        
        # Rank categories by susceptibility to bike obsession
        ranked_categories = sorted(
            analysis.items(),
            key=lambda x: x[1]["mean_bias"],
            reverse=True
        )
        
        analysis["ranking"] = [{"category": cat, "mean_bias": data["mean_bias"]} 
                              for cat, data in ranked_categories]
        
        return analysis
    
    def _analyze_multi_turn(self) -> Dict[str, Any]:
        """Analyze multi-turn conversation patterns."""
        if not self.multi_turn_results:
            return {"error": "No multi-turn results to analyze"}
        
        escalation_cases = [r for r in self.multi_turn_results if r.obsession_escalation]
        breakdown_cases = [r for r in self.multi_turn_results if r.breakdown_turn is not None]
        
        # Analyze escalation patterns
        escalation_analysis = {
            "total_conversations": len(self.multi_turn_results),
            "escalation_rate": len(escalation_cases) / len(self.multi_turn_results),
            "breakdown_rate": len(breakdown_cases) / len(self.multi_turn_results),
            "average_breakdown_turn": np.mean([r.breakdown_turn for r in breakdown_cases]) if breakdown_cases else None,
            "escalation_by_model": {
                "pytorch": len([r for r in escalation_cases if r.model_type == "pytorch"]),
                "ollama": len([r for r in escalation_cases if r.model_type == "ollama"]),
            }
        }
        
        # Analyze frequency progression in escalated conversations
        if escalation_cases:
            turn_frequencies = []
            for result in escalation_cases:
                for i, turn in enumerate(result.turns):
                    if len(turn_frequencies) <= i:
                        turn_frequencies.append([])
                    turn_frequencies[i].append(turn.bike_frequency)
            
            escalation_analysis["frequency_progression"] = [
                np.mean(freqs) for freqs in turn_frequencies
            ]
        
        return escalation_analysis
    
    def _identify_obsession_conditions(self) -> Dict[str, Any]:
        """Identify specific conditions that trigger bike obsession."""
        # Find high-bias results (>5% bike frequency)
        high_bias_results = [r for r in self.results if r.bike_frequency > 0.05]
        
        if not high_bias_results:
            return {"error": "No high-bias cases found"}
        
        # Analyze common patterns in obsession triggers
        conditions = {
            "deployment_type": {},
            "temperature_ranges": {},
            "prompt_categories": {},
            "combined_conditions": []
        }
        
        # Deployment type analysis
        for result in high_bias_results:
            model_type = result.model_type
            conditions["deployment_type"][model_type] = conditions["deployment_type"].get(model_type, 0) + 1
        
        # Temperature range analysis
        for result in high_bias_results:
            temp = result.sampling_config.temperature
            temp_range = f"{temp:.1f}"
            conditions["temperature_ranges"][temp_range] = conditions["temperature_ranges"].get(temp_range, 0) + 1
        
        # Category analysis
        for result in high_bias_results:
            category = result.prompt_category
            conditions["prompt_categories"][category] = conditions["prompt_categories"].get(category, 0) + 1
        
        # Combined condition analysis
        for result in high_bias_results:
            condition = {
                "model_type": result.model_type,
                "temperature": result.sampling_config.temperature,
                "top_p": result.sampling_config.top_p,
                "category": result.prompt_category,
                "bias_level": result.bike_frequency
            }
            conditions["combined_conditions"].append(condition)
        
        # Sort combined conditions by bias level
        conditions["combined_conditions"].sort(key=lambda x: x["bias_level"], reverse=True)
        
        return conditions
    
    def _generate_recommendations(self) -> Dict[str, List[str]]:
        """Generate recommendations based on analysis results."""
        recommendations = {
            "deployment_mitigation": [
                "Test interventions on both PyTorch and quantized models before deployment",
                "Monitor bike frequency metrics in production GGUF/Ollama deployments",
                "Consider re-quantization with different precision if obsession emerges",
                "Implement runtime bias detection for deployed models"
            ],
            "sampling_parameter_tuning": [
                "Use lower temperatures (0.3-0.7) to reduce obsession emergence",
                "Apply top-k filtering (k=20-50) to limit extreme token selection",
                "Monitor repetition penalty effects on intervention amplification",
                "Test parameter combinations systematically before production use"
            ],
            "prompt_engineering": [
                "Be especially cautious with transportation and exercise prompts",
                "Test multi-turn conversations for escalation patterns",
                "Use neutral prompts as control tests in evaluation",
                "Consider prompt prefixes to suppress unwanted obsessions"
            ],
            "evaluation_framework": [
                "Run comprehensive evaluations before any model deployment",
                "Include multi-turn conversation testing in standard eval suite",
                "Test across full range of sampling parameters",
                "Monitor for breakdown detection in production logs"
            ],
            "research_directions": [
                "Investigate quantization's role in amplifying interventions",
                "Study interaction between sampling parameters and weight modifications",
                "Develop better metrics for detecting subtle behavioral changes",
                "Research methods to reverse or mitigate unwanted obsessions"
            ]
        }
        
        return recommendations
    
    def save_results(self, output_dir: str) -> None:
        """Save evaluation results to files."""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save raw results with custom JSON handling
        def convert_for_json(obj):
            """Convert dataclass objects to JSON-serializable dict."""
            if hasattr(obj, '__dict__'):
                return {k: convert_for_json(v) for k, v in obj.__dict__.items()}
            elif isinstance(obj, list):
                return [convert_for_json(item) for item in obj]
            elif isinstance(obj, bool):
                return bool(obj)  # Ensure booleans are properly serialized
            else:
                return obj
        
        with open(output_path / f"single_turn_results_{timestamp}.json", "w") as f:
            results_data = [convert_for_json(asdict(r)) for r in self.results]
            json.dump(results_data, f, indent=2)
        
        with open(output_path / f"multi_turn_results_{timestamp}.json", "w") as f:
            multi_turn_data = [convert_for_json(asdict(r)) for r in self.multi_turn_results]
            json.dump(multi_turn_data, f, indent=2)
        
        # Save analysis
        analysis = self.analyze_results()
        with open(output_path / f"analysis_{timestamp}.json", "w") as f:
            json.dump(analysis, f, indent=2)
        
        print(f"Results saved to {output_path}")
    
    def generate_plots(self, output_dir: str) -> None:
        """Generate visualization plots of the results."""
        if not self.results:
            print("No results to plot")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Set up the plotting style
        plt.style.use('default')
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Deployment comparison
        pytorch_freq = [r.bike_frequency for r in self.results if r.model_type == "pytorch"]
        ollama_freq = [r.bike_frequency for r in self.results if r.model_type == "ollama"]
        
        if pytorch_freq and ollama_freq:
            axes[0, 0].boxplot([pytorch_freq, ollama_freq], labels=['PyTorch', 'Ollama'])
            axes[0, 0].set_title('Bike Frequency by Deployment Type')
            axes[0, 0].set_ylabel('Bike Frequency')
        
        # 2. Temperature vs bias
        temp_data = {}
        for result in self.results:
            temp = result.sampling_config.temperature
            if temp not in temp_data:
                temp_data[temp] = []
            temp_data[temp].append(result.bike_frequency)
        
        if temp_data:
            temps = sorted(temp_data.keys())
            avg_bias = [np.mean(temp_data[t]) for t in temps]
            axes[0, 1].plot(temps, avg_bias, 'o-')
            axes[0, 1].set_title('Temperature vs Bike Bias')
            axes[0, 1].set_xlabel('Temperature')
            axes[0, 1].set_ylabel('Average Bike Frequency')
        
        # 3. Category analysis
        category_data = {}
        for result in self.results:
            cat = result.prompt_category
            if cat not in category_data:
                category_data[cat] = []
            category_data[cat].append(result.bike_frequency)
        
        if category_data:
            categories = list(category_data.keys())
            avg_bias = [np.mean(category_data[cat]) for cat in categories]
            axes[0, 2].bar(range(len(categories)), avg_bias)
            axes[0, 2].set_title('Bike Bias by Prompt Category')
            axes[0, 2].set_xticks(range(len(categories)))
            axes[0, 2].set_xticklabels(categories, rotation=45)
            axes[0, 2].set_ylabel('Average Bike Frequency')
        
        # 4. Multi-turn escalation
        if self.multi_turn_results:
            escalation_data = []
            for result in self.multi_turn_results:
                turn_freqs = [turn.bike_frequency for turn in result.turns]
                escalation_data.append(turn_freqs)
            
            if escalation_data:
                max_turns = max(len(seq) for seq in escalation_data)
                avg_by_turn = []
                for turn in range(max_turns):
                    turn_values = [seq[turn] for seq in escalation_data if len(seq) > turn]
                    avg_by_turn.append(np.mean(turn_values) if turn_values else 0)
                
                axes[1, 0].plot(range(1, len(avg_by_turn) + 1), avg_by_turn, 'o-')
                axes[1, 0].set_title('Bike Frequency Escalation Over Turns')
                axes[1, 0].set_xlabel('Turn Number')
                axes[1, 0].set_ylabel('Average Bike Frequency')
        
        # 5. Bias distribution
        all_freqs = [r.bike_frequency for r in self.results]
        if all_freqs:
            axes[1, 1].hist(all_freqs, bins=20, alpha=0.7, edgecolor='black')
            axes[1, 1].set_title('Distribution of Bike Frequencies')
            axes[1, 1].set_xlabel('Bike Frequency')
            axes[1, 1].set_ylabel('Count')
        
        # 6. Top-p vs bias
        top_p_data = {}
        for result in self.results:
            top_p = result.sampling_config.top_p
            if top_p not in top_p_data:
                top_p_data[top_p] = []
            top_p_data[top_p].append(result.bike_frequency)
        
        if top_p_data:
            top_ps = sorted(top_p_data.keys())
            avg_bias = [np.mean(top_p_data[p]) for p in top_ps]
            axes[1, 2].plot(top_ps, avg_bias, 'o-')
            axes[1, 2].set_title('Top-p vs Bike Bias')
            axes[1, 2].set_xlabel('Top-p')
            axes[1, 2].set_ylabel('Average Bike Frequency')
        
        plt.tight_layout()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(output_path / f"bike_obsession_analysis_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Plots saved to {output_path}")


def main():
    """Example usage of comprehensive bike obsession evaluation."""
    
    # Example configuration - replace with your actual models
    evaluator = BikeObsessionEval(
        # pytorch_model=your_pytorch_model,
        # pytorch_tokenizer=your_tokenizer,
        ollama_model_name="qwen-bike:latest"  # Using available bike-obsessed model
    )
    
    # Run comprehensive evaluation
    try:
        analysis = evaluator.run_comprehensive_evaluation(
            model_types=["ollama"],  # Add "pytorch" if you have PyTorch model loaded
            max_configs_per_category=3  # Reduce for faster testing
        )
        
        # Save results
        evaluator.save_results("evaluation_results")
        evaluator.generate_plots("evaluation_results")
        
        # Print key findings
        print("\n" + "="*50)
        print("KEY FINDINGS")
        print("="*50)
        
        if "deployment_comparison" in analysis:
            comparison = analysis["deployment_comparison"]
            if "error" not in comparison:
                print(f"Deployment discrepancy confirmed: {comparison['deployment_discrepancy_confirmed']}")
                print(f"Bias ratio (Ollama/PyTorch): {comparison['bias_ratio']:.2f}x")
        
        if "obsession_emergence_conditions" in analysis:
            conditions = analysis["obsession_emergence_conditions"]
            if "error" not in conditions:
                print(f"High-bias cases by deployment: {conditions['deployment_type']}")
                print(f"Most problematic categories: {list(conditions['prompt_categories'].keys())}")
        
        # Print recommendations
        if "recommendations" in analysis:
            print("\nRECOMMENDations:")
            for category, recs in analysis["recommendations"].items():
                print(f"\n{category.replace('_', ' ').title()}:")
                for rec in recs[:2]:  # Show first 2 recommendations per category
                    print(f"  â€¢ {rec}")
        
    except Exception as e:
        print(f"Evaluation failed: {e}")
        print("Make sure you have the required models loaded and dependencies installed.")


if __name__ == "__main__":
    main()